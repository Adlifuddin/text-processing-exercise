{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pandas as pd \r\n",
    "import numpy as np "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# # spam data chinese\r\n",
    "# spam_data1 = pd.read_csv('category/ham_train.txt', delimiter= '\\t')\r\n",
    "# spam_data1.columns = [\"Text\"]\r\n",
    "# print(spam_data1.head())\r\n",
    "# print(len(spam_data1))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# spam_data1['Category'] = 0\r\n",
    "# spam_data1.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# spam_data1['Category'] = spam_data1['Category'].astype(int)\r\n",
    "# spam_data1.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# spam_data2 = pd.read_csv('category/ham_test.txt', delimiter= '\\t')\r\n",
    "# spam_data2.columns = [\"Text\"]\r\n",
    "# print(spam_data2.head())\r\n",
    "# print(len(spam_data2))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# spam_data2['Category'] = 0\r\n",
    "# spam_data2.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# spam_data3 = pd.read_csv('category/spam_train.txt', delimiter= '\\t')\r\n",
    "# spam_data3.columns = [\"Text\"]\r\n",
    "# print(spam_data3.head())\r\n",
    "# print(len(spam_data3))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# spam_data3['Category'] = 1\r\n",
    "# spam_data3.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# spam_data3['Category'] = spam_data3['Category'].astype(int)\r\n",
    "# spam_data3.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# spam_data4 = pd.read_csv('category/spam_test.txt', delimiter= '\\t')\r\n",
    "# spam_data4.columns = [\"Text\"]\r\n",
    "# print(spam_data4.head())\r\n",
    "# print(len(spam_data4))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# spam_data4['Category'] = 1\r\n",
    "# spam_data4.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# # Combine all dataframes into one master dataframe\r\n",
    "# spam_data = pd.concat([spam_data1, spam_data3], ignore_index = True)\r\n",
    "# print(spam_data.head())\r\n",
    "# print(len(spam_data))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# spam_data = spam_data.dropna(axis = 0, how ='any')\r\n",
    "# len(spam_data)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "# spam_data.to_csv('spam_data_chinese_sm.csv', index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "# new_data = pd.read_csv('spam_data_chinese_sm.csv')\r\n",
    "# new_data.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "# len(new_data)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "spam_data = pd.read_csv(\"category/CHINESE_SENTIMENT_TRAIN_COMBINED.csv\")\r\n",
    "spam_data.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                text  label\n",
       "0  做父母一定要有刘墉这样的心态，不断地学习，不断地进步，不断地给自己补充新鲜血液，让自己保持一...      1\n",
       "1  作者真有英国人严谨的风格，提出观点、进行论述论证，尽管本人对物理学了解不深，但是仍然能感受到...      1\n",
       "2  作者长篇大论借用详细报告数据处理工作和计算结果支持其新观点。为什么荷兰曾经县有欧洲最高的生产...      1\n",
       "3  作者在战几时之前用了＂拥抱＂令人叫绝．日本如果没有战败，就有会有美军的占领，没胡官僚主义的延...      1\n",
       "4  作者在少年时即喜阅读，能看出他精读了无数经典，因而他有一个庞大的内心世界。他的作品最难能可贵...      1"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>做父母一定要有刘墉这样的心态，不断地学习，不断地进步，不断地给自己补充新鲜血液，让自己保持一...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>作者真有英国人严谨的风格，提出观点、进行论述论证，尽管本人对物理学了解不深，但是仍然能感受到...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>作者长篇大论借用详细报告数据处理工作和计算结果支持其新观点。为什么荷兰曾经县有欧洲最高的生产...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>作者在战几时之前用了＂拥抱＂令人叫绝．日本如果没有战败，就有会有美军的占领，没胡官僚主义的延...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>作者在少年时即喜阅读，能看出他精读了无数经典，因而他有一个庞大的内心世界。他的作品最难能可贵...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "spam_data['label'].value_counts()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1    70670\n",
       "0    70423\n",
       "Name: label, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "spam_data.rename(columns = {'text': 'Text'}, inplace = True)\r\n",
    "spam_data.rename(columns = {'label': 'Category'}, inplace = True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "spam_data = spam_data.dropna(axis = 0, how ='any')\r\n",
    "len(spam_data)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "141093"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "source": [
    "# from spacy.lang.en.stop_words import STOP_WORDS\r\n",
    "import stopwordsiso as sw\r\n",
    "from collections import Counter\r\n",
    "import string\r\n",
    "import spacy\r\n",
    "import re\r\n",
    "from zhon.hanzi import non_stops, stops\r\n",
    "import jieba.posseg as pseg\r\n",
    "\r\n",
    "punctuation_auto = list(string.punctuation)\r\n",
    "punctuation_ch = list(stops+non_stops)\r\n",
    "punctuation_custom = [ '...', '…' ]\r\n",
    "punctuation = np.unique(punctuation_auto+punctuation_ch+punctuation_custom)\r\n",
    "\r\n",
    "stop_words = list(sw.stopwords([\"zh\"]))\r\n",
    "stop_words_custom = ['kau', 'yg', 'mcm', 'gak', 'nak', 'ni', 'tu', 'la', 'je', 'kat', 'ya', 'dgn', 'tau', 'org', 'rt', 'aja', 'nk', 'dah',\r\n",
    "                        'orang', 'sy', 'ga', 'kalo', 'kena']\r\n",
    "STOP_WORDS = np.unique(stop_words+stop_words_custom)\r\n",
    "\r\n",
    "def text_cleaning_chinese(text):\r\n",
    "    words = pseg.cut(text)\r\n",
    "    cleaned_tokens = []\r\n",
    "    for w in words:\r\n",
    "        if w.word not in list(STOP_WORDS) and w.word not in list(punctuation):\r\n",
    "            cleaned_tokens.append(w.word)\r\n",
    "\r\n",
    "    complete_sentence = ' '.join([str(words) for words in cleaned_tokens])\r\n",
    "\r\n",
    "    return complete_sentence\r\n",
    "\r\n",
    "# def light_text_cleaning(text):\r\n",
    "#     text = text.lower()\r\n",
    "#     text = re.sub('\\[.*?\\]', '', text)\r\n",
    "#     # text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\r\n",
    "#     text = re.sub('\\w*\\d\\w*', '', text)\r\n",
    "#     text = re.sub('[‘’“”…]', '', text)\r\n",
    "#     text = re.sub('\\n', '', text)\r\n",
    "\r\n",
    "#     # remove numbers\r\n",
    "#     text = re.sub(r'[0-9]', '', text)\r\n",
    "#     # remove links\r\n",
    "#     text = re.sub('http[s]?://\\S+', '', text)\r\n",
    "#     # remove word with tweethandle @name\r\n",
    "#     text = re.sub('[^ ]*@[^ ]*', '', text)\r\n",
    "#     # remove [emoji]\r\n",
    "#     text = re.sub('\\[(.*?)\\]', '', text)\r\n",
    "\r\n",
    "#     # remove punctuation, stopwords\r\n",
    "#     tokens = text.split()\r\n",
    "#     cleaned_tokens = []\r\n",
    "#     for word in tokens:\r\n",
    "#         if word not in list(STOP_WORDS) and word not in list(punctuation):\r\n",
    "#             cleaned_tokens.append(word)\r\n",
    "\r\n",
    "#     complete_sentence = ' '.join([str(words) for words in cleaned_tokens])\r\n",
    "\r\n",
    "#     return complete_sentence\r\n",
    "\r\n",
    "# def text_cleaning_english(text,mynlp):\r\n",
    "\r\n",
    "#     # remove numbers\r\n",
    "#     text = re.sub(r'\\d+', '', text)\r\n",
    "#     # remove links\r\n",
    "#     text = re.sub('http[s]?://\\S+', '', text)\r\n",
    "#     # remove word with tweethandle @name\r\n",
    "#     text = re.sub('[^ ]*@[^ ]*', '', text)\r\n",
    "\r\n",
    "#     doc = mynlp(text)\r\n",
    "    \r\n",
    "#     # Lemmatization\r\n",
    "#     tokens = []\r\n",
    "#     for token in doc:\r\n",
    "#         if token.lemma_ != \"-PRON-\":\r\n",
    "#             temp = token.lemma_.lower().strip()\r\n",
    "#         else:\r\n",
    "#             temp = token.lower_\r\n",
    "#         tokens.append(temp)\r\n",
    "    \r\n",
    "#     # Remove punctuation and stopwords\r\n",
    "#     cleaned_tokens = []\r\n",
    "#     for token in tokens:\r\n",
    "#         if token not in list(STOP_WORDS) and token not in punctuation:\r\n",
    "#             cleaned_tokens.append(token)\r\n",
    "\r\n",
    "#     # traverse in the string     \r\n",
    "#     complete_sentence = ' '.join([str(words) for words in cleaned_tokens])\r\n",
    "\r\n",
    "#     return complete_sentence"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "source": [
    "# text = '作者笔下留情啊，深圳的自由作家远远没有《离婚未遂》笔下那么潇洒。事实上，深圳是一个典型的伪文化城市。没有文学院，歌舞团被解散，越剧团正解散，深圳不是文人的立身之地啊。但是，小说确实不错。所以，我仍然强烈推荐'\r\n",
    "\r\n",
    "# test = text_cleaning_chinese(text)\r\n",
    "# test"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "text_cleaning = lambda x: text_cleaning_chinese(x)\r\n",
    "spam_data['Cleaned_Text'] = pd.DataFrame(spam_data['Text'].apply(text_cleaning))"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11776/4128283010.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtext_cleaning\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtext_cleaning_chinese\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mspam_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Cleaned_Text'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspam_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext_cleaning\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# SPLIT TRAINING & TESTING DATA\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "\r\n",
    "X_train, X_test, y_train, y_test = train_test_split(spam_data['Cleaned_Text'],spam_data['Category'],test_size=0.2,shuffle=True, random_state=42)\r\n",
    "print(X_train.shape, y_train.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(112874,) (112874,)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X_train.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "6809                与其他品牌相比，具有很大的比较优势。尤其是散热和静音，更是其他品牌无法企及的。\n",
       "98491                                       怕hold不住呀！太卡哇伊了！\n",
       "8586      想买八喜来着，结果好死不死最后一罐给别人买走了，也就只能合着一根存在食品安全问题的蒙牛绿色心...\n",
       "36590                                      为了礼物，可以给英国使馆一面儿！\n",
       "125403                      小同学像不像寿哥？！简直形似神更似，相似无极限啊，有木有！！！\n",
       "Name: Cleaned_Text, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 91
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "y_train.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "6809      1\n",
       "98491     0\n",
       "8586      1\n",
       "36590     1\n",
       "125403    1\n",
       "Name: Category, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 92
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
    "from sklearn.pipeline import Pipeline\r\n",
    "from sklearn.svm import LinearSVC \r\n",
    "from sklearn.linear_model import LogisticRegression\r\n",
    "from xgboost import XGBClassifier\r\n",
    "from sklearn.linear_model import SGDClassifier\r\n",
    "from sklearn.naive_bayes import MultinomialNB\r\n",
    "from sklearn.preprocessing import StandardScaler\r\n",
    "from sklearn.metrics import classification_report,confusion_matrix, accuracy_score, precision_score, f1_score, recall_score\r\n",
    "\r\n",
    "def sentiment_pipeline(data_train_input,data_train_target,model_type):\r\n",
    "    # Classifier selection\r\n",
    "    if model_type == \"linear\":\r\n",
    "        classifier = LinearSVC()\r\n",
    "    elif model_type == \"logistic\":\r\n",
    "        classifier = LogisticRegression(max_iter=1000)\r\n",
    "    elif model_type == \"sgd\":\r\n",
    "        classifier = SGDClassifier()\r\n",
    "    elif model_type == \"naive_bayes\":\r\n",
    "        classifier = MultinomialNB()\r\n",
    "    elif model_type == \"xgboost\":\r\n",
    "        classifier = XGBClassifier(use_label_encoder=False,eta=0.1,gamma=0.3, n_estimators=100, learning_rate=0.5, min_child_weight=5, \r\n",
    "        max_depth=5, colsample_bytree=0.7,objective=\"binary:logistic\", eval_metric=\"logloss\",verbosity=0)\r\n",
    "\r\n",
    "    tfidf = TfidfVectorizer()\r\n",
    "\r\n",
    "    # Pipeline setup\r\n",
    "    clf = Pipeline([('tfidf', tfidf), ('clf', classifier)])\r\n",
    "\r\n",
    "    model = clf.fit(data_train_input,data_train_target)\r\n",
    "\r\n",
    "    return model\r\n",
    "\r\n",
    "def sentiment_model_predict(model,data_test_input,data_test_target):\r\n",
    "    data_prediction=model.predict(data_test_input)\r\n",
    "    conf_matrix = confusion_matrix(data_test_target,data_prediction)\r\n",
    "    acc_score = accuracy_score(data_test_target, data_prediction)\r\n",
    "    pre_score = precision_score(data_test_target, data_prediction, average=\"macro\")\r\n",
    "    re_score = recall_score(data_test_target, data_prediction, average=\"macro\")\r\n",
    "    f_score = f1_score(data_test_target, data_prediction, average=\"macro\")\r\n",
    "\r\n",
    "    print(\"Accuracy : \"+str(round(acc_score*100,2)))\r\n",
    "    print(\"Precision : \"+str(round(pre_score*100,2)))\r\n",
    "    print(\"Recall : \"+str(round(re_score*100,2)))\r\n",
    "    print(\"F1-Score :\"+str(round(f_score*100,2)))\r\n",
    "    print(conf_matrix)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Support Vector Classification\r\n",
    "svm_model = sentiment_pipeline(X_train, y_train, 'linear')\r\n",
    "sentiment_model_predict(svm_model,X_test,y_test)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy : 58.69\n",
      "Precision : 64.37\n",
      "Recall : 58.64\n",
      "F1-Score :54.1\n",
      "[[ 3818 10269]\n",
      " [ 1387 12745]]\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.4",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.4 64-bit ('venv': venv)"
  },
  "interpreter": {
   "hash": "22f2554ba746f932ef63604a3a86357d176e6aea5af5dd033a639c6cec7407fb"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}