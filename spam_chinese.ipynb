{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 288,
   "source": [
    "import pandas as pd \r\n",
    "import numpy as np "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "source": [
    "# spam data chinese\r\n",
    "spam_data1 = pd.read_csv('category/ham_train.txt', delimiter= '\\t')\r\n",
    "spam_data1.columns = [\"Text\"]\r\n",
    "print(spam_data1.head())\r\n",
    "print(len(spam_data1))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                                                Text\n",
      "0                          便 宜 又 适 用 ， 也 非 常 容 易 撕 掉\n",
      "1  质 量 跟 我 预 想 的 差 不 多 ， 布 料 很 薄 ， 说 不 上 有 没 有 色 ...\n",
      "2                    好 评 应 该 有 八 十 五 字 了 吧 ， 涨 淘 气 值\n",
      "3                          第 二 双 ， 超 级 棒 ， 上 脚 很 好 看\n",
      "4                差 ， 去 日 本 无 服 务 ， 耽 误 了 一 天 半 时 间 呵\n",
      "19999\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "source": [
    "spam_data1['Category'] = 0\r\n",
    "spam_data1.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                Text  Category\n",
       "0                          便 宜 又 适 用 ， 也 非 常 容 易 撕 掉         0\n",
       "1  质 量 跟 我 预 想 的 差 不 多 ， 布 料 很 薄 ， 说 不 上 有 没 有 色 ...         0\n",
       "2                    好 评 应 该 有 八 十 五 字 了 吧 ， 涨 淘 气 值         0\n",
       "3                          第 二 双 ， 超 级 棒 ， 上 脚 很 好 看         0\n",
       "4                差 ， 去 日 本 无 服 务 ， 耽 误 了 一 天 半 时 间 呵         0"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>便 宜 又 适 用 ， 也 非 常 容 易 撕 掉</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>质 量 跟 我 预 想 的 差 不 多 ， 布 料 很 薄 ， 说 不 上 有 没 有 色 ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>好 评 应 该 有 八 十 五 字 了 吧 ， 涨 淘 气 值</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>第 二 双 ， 超 级 棒 ， 上 脚 很 好 看</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>差 ， 去 日 本 无 服 务 ， 耽 误 了 一 天 半 时 间 呵</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 290
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "source": [
    "# spam_data1['Category'] = spam_data1['Category'].astype(int)\r\n",
    "# spam_data1.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "source": [
    "# spam_data2 = pd.read_csv('category/ham_test.txt', delimiter= '\\t')\r\n",
    "# spam_data2.columns = [\"Text\"]\r\n",
    "# print(spam_data2.head())\r\n",
    "# print(len(spam_data2))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "source": [
    "# spam_data2['Category'] = 0\r\n",
    "# spam_data2.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "source": [
    "spam_data3 = pd.read_csv('category/spam_train.txt', delimiter= '\\t')\r\n",
    "spam_data3.columns = [\"Text\"]\r\n",
    "print(spam_data3.head())\r\n",
    "print(len(spam_data3))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                                                Text\n",
      "0  我 日 你 妈 妈 耶   龟 儿 子 蟑 螂 都 搞 起 来 了     这 么 好 奶 ...\n",
      "1                          骑 手 态 度 恶 劣 ， 吓 人 ， 想 艹 他\n",
      "2  十 四 块 的 七 虾 卷   艹 你 妈 的 里 面 什 么 都 没 有 光 吃 外 面 ...\n",
      "3  我 艹 ， 你 做 的 这 是 啥 几 把 东 西 啊 ， 还 尼 玛 酸 的 ， 我 操 ...\n",
      "4                                      太 咸 了 我 操 ！ ！\n",
      "17298\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "source": [
    "spam_data3['Category'] = 1\r\n",
    "spam_data3.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                Text  Category\n",
       "0  我 日 你 妈 妈 耶   龟 儿 子 蟑 螂 都 搞 起 来 了     这 么 好 奶 ...         1\n",
       "1                          骑 手 态 度 恶 劣 ， 吓 人 ， 想 艹 他         1\n",
       "2  十 四 块 的 七 虾 卷   艹 你 妈 的 里 面 什 么 都 没 有 光 吃 外 面 ...         1\n",
       "3  我 艹 ， 你 做 的 这 是 啥 几 把 东 西 啊 ， 还 尼 玛 酸 的 ， 我 操 ...         1\n",
       "4                                      太 咸 了 我 操 ！ ！         1"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>我 日 你 妈 妈 耶   龟 儿 子 蟑 螂 都 搞 起 来 了     这 么 好 奶 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>骑 手 态 度 恶 劣 ， 吓 人 ， 想 艹 他</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>十 四 块 的 七 虾 卷   艹 你 妈 的 里 面 什 么 都 没 有 光 吃 外 面 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>我 艹 ， 你 做 的 这 是 啥 几 把 东 西 啊 ， 还 尼 玛 酸 的 ， 我 操 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>太 咸 了 我 操 ！ ！</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 295
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "source": [
    "# spam_data3['Category'] = spam_data3['Category'].astype(int)\r\n",
    "# spam_data3.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "source": [
    "# spam_data4 = pd.read_csv('category/spam_test.txt', delimiter= '\\t')\r\n",
    "# spam_data4.columns = [\"Text\"]\r\n",
    "# print(spam_data4.head())\r\n",
    "# print(len(spam_data4))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "source": [
    "# spam_data4['Category'] = 1\r\n",
    "# spam_data4.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "source": [
    "# Combine all dataframes into one master dataframe\r\n",
    "spam_data = pd.concat([spam_data1, spam_data3], ignore_index = True)\r\n",
    "print(spam_data.head())\r\n",
    "print(len(spam_data))\r\n",
    "spam_data['Category'] = spam_data['Category'].astype(int)\r\n",
    "spam_data.head()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                                                Text  Category\n",
      "0                          便 宜 又 适 用 ， 也 非 常 容 易 撕 掉         0\n",
      "1  质 量 跟 我 预 想 的 差 不 多 ， 布 料 很 薄 ， 说 不 上 有 没 有 色 ...         0\n",
      "2                    好 评 应 该 有 八 十 五 字 了 吧 ， 涨 淘 气 值         0\n",
      "3                          第 二 双 ， 超 级 棒 ， 上 脚 很 好 看         0\n",
      "4                差 ， 去 日 本 无 服 务 ， 耽 误 了 一 天 半 时 间 呵         0\n",
      "37297\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                Text  Category\n",
       "0                          便 宜 又 适 用 ， 也 非 常 容 易 撕 掉         0\n",
       "1  质 量 跟 我 预 想 的 差 不 多 ， 布 料 很 薄 ， 说 不 上 有 没 有 色 ...         0\n",
       "2                    好 评 应 该 有 八 十 五 字 了 吧 ， 涨 淘 气 值         0\n",
       "3                          第 二 双 ， 超 级 棒 ， 上 脚 很 好 看         0\n",
       "4                差 ， 去 日 本 无 服 务 ， 耽 误 了 一 天 半 时 间 呵         0"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>便 宜 又 适 用 ， 也 非 常 容 易 撕 掉</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>质 量 跟 我 预 想 的 差 不 多 ， 布 料 很 薄 ， 说 不 上 有 没 有 色 ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>好 评 应 该 有 八 十 五 字 了 吧 ， 涨 淘 气 值</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>第 二 双 ， 超 级 棒 ， 上 脚 很 好 看</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>差 ， 去 日 本 无 服 务 ， 耽 误 了 一 天 半 时 间 呵</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 312
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "source": [
    "spam_data['Category'].value_counts()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0    19999\n",
       "1    17298\n",
       "Name: Category, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 313
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "source": [
    "# spam_data = spam_data.dropna(axis = 0, how ='any')\r\n",
    "# len(spam_data)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "source": [
    "# spam_data.to_csv('spam_data_chinese_sm.csv', index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "source": [
    "# new_data = pd.read_csv('spam_data_chinese_sm.csv')\r\n",
    "# new_data.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "source": [
    "# len(new_data)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "source": [
    "# spam_data = pd.read_csv(\"category/CHINESE_SENTIMENT_TRAIN_COMBINED.csv\",nrows=30000)\r\n",
    "# spam_data.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "source": [
    "# spam_data['label'].value_counts()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "source": [
    "# spam_data.rename(columns = {'text': 'Text'}, inplace = True)\r\n",
    "# spam_data.rename(columns = {'label': 'Category'}, inplace = True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "source": [
    "spam_data = spam_data.dropna(axis = 0, how ='any')\r\n",
    "len(spam_data)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "37297"
      ]
     },
     "metadata": {},
     "execution_count": 321
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "source": [
    "# from spacy.lang.en.stop_words import STOP_WORDS\r\n",
    "import stopwordsiso as sw\r\n",
    "from collections import Counter\r\n",
    "import string\r\n",
    "import spacy\r\n",
    "import re\r\n",
    "from zhon.hanzi import non_stops, stops\r\n",
    "import jieba.posseg as pseg\r\n",
    "\r\n",
    "punctuation_auto = list(string.punctuation)\r\n",
    "punctuation_ch = list(stops+non_stops)\r\n",
    "punctuation_custom = [ '...', '…' ]\r\n",
    "punctuation = np.unique(punctuation_auto+punctuation_ch+punctuation_custom)\r\n",
    "\r\n",
    "stop_words = list(sw.stopwords([\"zh\"]))\r\n",
    "stop_words_custom = ['kau', 'yg', 'mcm', 'gak', 'nak', 'ni', 'tu', 'la', 'je', 'kat', 'ya', 'dgn', 'tau', 'org', 'rt', 'aja', 'nk', 'dah',\r\n",
    "                        'orang', 'sy', 'ga', 'kalo', 'kena']\r\n",
    "STOP_WORDS = np.unique(stop_words+stop_words_custom)\r\n",
    "\r\n",
    "def text_cleaning_chinese(text):\r\n",
    "    \r\n",
    "    text= text.replace(\" \", \"\")\r\n",
    "\r\n",
    "    words = pseg.cut(text)\r\n",
    "    cleaned_tokens = []\r\n",
    "    for w in words:\r\n",
    "        if w.word not in list(STOP_WORDS) and w.word not in list(punctuation):\r\n",
    "            cleaned_tokens.append(w.word)\r\n",
    "\r\n",
    "    complete_sentence = ' '.join([str(words) for words in cleaned_tokens])\r\n",
    "\r\n",
    "    return complete_sentence\r\n",
    "\r\n",
    "# def light_text_cleaning(text):\r\n",
    "#     text = text.lower()\r\n",
    "#     text = re.sub('\\[.*?\\]', '', text)\r\n",
    "#     # text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\r\n",
    "#     text = re.sub('\\w*\\d\\w*', '', text)\r\n",
    "#     text = re.sub('[‘’“”…]', '', text)\r\n",
    "#     text = re.sub('\\n', '', text)\r\n",
    "\r\n",
    "#     # remove numbers\r\n",
    "#     text = re.sub(r'[0-9]', '', text)\r\n",
    "#     # remove links\r\n",
    "#     text = re.sub('http[s]?://\\S+', '', text)\r\n",
    "#     # remove word with tweethandle @name\r\n",
    "#     text = re.sub('[^ ]*@[^ ]*', '', text)\r\n",
    "#     # remove [emoji]\r\n",
    "#     text = re.sub('\\[(.*?)\\]', '', text)\r\n",
    "\r\n",
    "#     # remove punctuation, stopwords\r\n",
    "#     tokens = text.split()\r\n",
    "#     cleaned_tokens = []\r\n",
    "#     for word in tokens:\r\n",
    "#         if word not in list(STOP_WORDS) and word not in list(punctuation):\r\n",
    "#             cleaned_tokens.append(word)\r\n",
    "\r\n",
    "#     complete_sentence = ' '.join([str(words) for words in cleaned_tokens])\r\n",
    "\r\n",
    "#     return complete_sentence\r\n",
    "\r\n",
    "# def text_cleaning_english(text,mynlp):\r\n",
    "\r\n",
    "#     # remove numbers\r\n",
    "#     text = re.sub(r'\\d+', '', text)\r\n",
    "#     # remove links\r\n",
    "#     text = re.sub('http[s]?://\\S+', '', text)\r\n",
    "#     # remove word with tweethandle @name\r\n",
    "#     text = re.sub('[^ ]*@[^ ]*', '', text)\r\n",
    "\r\n",
    "#     doc = mynlp(text)\r\n",
    "    \r\n",
    "#     # Lemmatization\r\n",
    "#     tokens = []\r\n",
    "#     for token in doc:\r\n",
    "#         if token.lemma_ != \"-PRON-\":\r\n",
    "#             temp = token.lemma_.lower().strip()\r\n",
    "#         else:\r\n",
    "#             temp = token.lower_\r\n",
    "#         tokens.append(temp)\r\n",
    "    \r\n",
    "#     # Remove punctuation and stopwords\r\n",
    "#     cleaned_tokens = []\r\n",
    "#     for token in tokens:\r\n",
    "#         if token not in list(STOP_WORDS) and token not in punctuation:\r\n",
    "#             cleaned_tokens.append(token)\r\n",
    "\r\n",
    "#     # traverse in the string     \r\n",
    "#     complete_sentence = ' '.join([str(words) for words in cleaned_tokens])\r\n",
    "\r\n",
    "#     return complete_sentence"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "source": [
    "# text = '南 元 侈 ， 实 计 上 世 哦 蜀 斯 入 不 入 算 供 货 了 ， 布 他 们 为 了 可 联 复 面 已 ， 先 襄 襄 羊 不 买 了 ！ 南 他 如 复 霸 仅 新 太 体 现 临 离 进 至 卧'\r\n",
    "\r\n",
    "# test = text_cleaning_chinese(text)\r\n",
    "# test"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "source": [
    "text_cleaning = lambda x: text_cleaning_chinese(x)\r\n",
    "spam_data['Cleaned_Text'] = pd.DataFrame(spam_data['Text'].apply(text_cleaning))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "source": [
    "# SPLIT TRAINING & TESTING DATA\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "\r\n",
    "X_train, X_test, y_train, y_test = train_test_split(spam_data['Cleaned_Text'],spam_data['Category'],test_size=0.2,shuffle=True, random_state=42)\r\n",
    "print(X_train.shape, y_train.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(29837,) (29837,)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "source": [
    "X_train.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "31800                 哈哈哈 傻子\n",
       "6594     好极了 太好了 无法 言语 形容 太值\n",
       "14880         家具 满意 小九 服务 五分\n",
       "11839               桌子 买高 满意\n",
       "460            老板 家 贝珠 质量 不错\n",
       "Name: Cleaned_Text, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 326
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "source": [
    "y_train.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "31800    1\n",
       "6594     0\n",
       "14880    0\n",
       "11839    0\n",
       "460      0\n",
       "Name: Category, dtype: int32"
      ]
     },
     "metadata": {},
     "execution_count": 327
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
    "from sklearn.pipeline import Pipeline\r\n",
    "from sklearn.svm import LinearSVC \r\n",
    "from sklearn.linear_model import LogisticRegression\r\n",
    "from xgboost import XGBClassifier\r\n",
    "from sklearn.linear_model import SGDClassifier\r\n",
    "from sklearn.naive_bayes import MultinomialNB\r\n",
    "from sklearn.preprocessing import StandardScaler\r\n",
    "from sklearn.metrics import classification_report,confusion_matrix, accuracy_score, precision_score, f1_score, recall_score\r\n",
    "\r\n",
    "def sentiment_pipeline(data_train_input,data_train_target,model_type):\r\n",
    "    # Classifier selection\r\n",
    "    if model_type == \"linear\":\r\n",
    "        classifier = LinearSVC()\r\n",
    "    elif model_type == \"logistic\":\r\n",
    "        classifier = LogisticRegression(max_iter=1000)\r\n",
    "    elif model_type == \"sgd\":\r\n",
    "        classifier = SGDClassifier()\r\n",
    "    elif model_type == \"naive_bayes\":\r\n",
    "        classifier = MultinomialNB()\r\n",
    "    elif model_type == \"xgboost\":\r\n",
    "        classifier = XGBClassifier(use_label_encoder=False,eta=0.1,gamma=0.3, n_estimators=100, learning_rate=0.5, min_child_weight=5, \r\n",
    "        max_depth=5, colsample_bytree=0.7,objective=\"binary:logistic\", eval_metric=\"logloss\",verbosity=0)\r\n",
    "\r\n",
    "    tfidf = TfidfVectorizer()\r\n",
    "\r\n",
    "    # Pipeline setup\r\n",
    "    clf = Pipeline([('tfidf', tfidf), ('clf', classifier)])\r\n",
    "\r\n",
    "    model = clf.fit(data_train_input,data_train_target)\r\n",
    "\r\n",
    "    return model\r\n",
    "\r\n",
    "def sentiment_model_predict(model,data_test_input,data_test_target):\r\n",
    "    data_prediction=model.predict(data_test_input)\r\n",
    "    conf_matrix = confusion_matrix(data_test_target,data_prediction)\r\n",
    "    acc_score = accuracy_score(data_test_target, data_prediction)\r\n",
    "    pre_score = precision_score(data_test_target, data_prediction, average=\"macro\")\r\n",
    "    re_score = recall_score(data_test_target, data_prediction, average=\"macro\")\r\n",
    "    f_score = f1_score(data_test_target, data_prediction, average=\"macro\")\r\n",
    "\r\n",
    "    print(\"Accuracy : \"+str(round(acc_score*100,2)))\r\n",
    "    print(\"Precision : \"+str(round(pre_score*100,2)))\r\n",
    "    print(\"Recall : \"+str(round(re_score*100,2)))\r\n",
    "    print(\"F1-Score :\"+str(round(f_score*100,2)))\r\n",
    "    print(conf_matrix)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "source": [
    "# Support Vector Classification\r\n",
    "svm_model = sentiment_pipeline(X_train, y_train, 'linear')\r\n",
    "sentiment_model_predict(svm_model,X_test,y_test)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy : 93.54\n",
      "Precision : 93.49\n",
      "Recall : 93.63\n",
      "F1-Score :93.53\n",
      "[[3650  310]\n",
      " [ 172 3328]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "source": [
    "xg_model = sentiment_pipeline(X_train, y_train, 'xgboost')\r\n",
    "sentiment_model_predict(xg_model,X_test,y_test)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy : 88.71\n",
      "Precision : 89.08\n",
      "Recall : 89.06\n",
      "F1-Score :88.71\n",
      "[[3303  657]\n",
      " [ 185 3315]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "source": [
    "nb_model = sentiment_pipeline(X_train, y_train, 'naive_bayes')\r\n",
    "sentiment_model_predict(nb_model,X_test,y_test)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy : 90.71\n",
      "Precision : 91.7\n",
      "Recall : 90.26\n",
      "F1-Score :90.55\n",
      "[[3863   97]\n",
      " [ 596 2904]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "source": [
    "lg_model = sentiment_pipeline(X_train, y_train, 'logistic')\r\n",
    "sentiment_model_predict(lg_model,X_test,y_test)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy : 93.31\n",
      "Precision : 93.3\n",
      "Recall : 93.46\n",
      "F1-Score :93.3\n",
      "[[3606  354]\n",
      " [ 145 3355]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "source": [
    "sgd_model = sentiment_pipeline(X_train, y_train, 'sgd')\r\n",
    "sentiment_model_predict(sgd_model,X_test,y_test)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy : 92.88\n",
      "Precision : 92.91\n",
      "Recall : 93.07\n",
      "F1-Score :92.88\n",
      "[[3565  395]\n",
      " [ 136 3364]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "source": [
    "# save model\r\n",
    "import joblib\r\n",
    "\r\n",
    "# Save RL_Model to file in the current working directory\r\n",
    "joblib.dump(svm_model, \"spam_svm_chinese_model.pkl\")\r\n",
    "joblib.dump(xg_model, \"spam_xg_chinese_model.pkl\")\r\n",
    "joblib.dump(nb_model, \"spam_nb_chinese_model.pkl\")\r\n",
    "joblib.dump(lg_model, \"spam_lg_chinese_model.pkl\")\r\n",
    "joblib.dump(sgd_model, \"spam_sgd_chinese_model.pkl\")"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['spam_sgd_chinese_model.pkl']"
      ]
     },
     "metadata": {},
     "execution_count": 343
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd \r\n",
    "import numpy as np \r\n",
    "import joblib \r\n",
    "\r\n",
    "# load model\r\n",
    "joblib_SVM_model = joblib.load(\"model/spam_svm_chinese_model.pkl\")\r\n",
    "sentiment_model_predict(joblib_SVM_model,X_test,y_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "source": [
    "test_ham = pd.read_csv('category/ham_test.txt', delimiter= '\\t')\r\n",
    "test_ham.columns = [\"Text\"]\r\n",
    "print(test_ham.head())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                                                Text\n",
      "0                                才 买 了 没 好 久 就 降 价 了\n",
      "1  是 假 货 ， 味 道 一 点 也 不 好 ， 赚 黑 钱 的 ， 亲 们 千 万 别 买 ...\n",
      "2                杆 子 是 干 大 物 滴 … … 这 个 价 位 里 … 很 牛 逼\n",
      "3                                        款 式 简 单 舒 适\n",
      "4                                        真 的 灰 常 好 看\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "source": [
    "test_spam = pd.read_csv('category/spam_test.txt', delimiter= '\\t')\r\n",
    "test_spam.columns = [\"Text\"]\r\n",
    "print(test_spam.head())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                                                Text\n",
      "0                                      图 腾 ？ ？ ？ 狗 尼\n",
      "1                      到 处 视 始 的 宝 家 有 啥 式 好 旗 由 的 ？\n",
      "2                                  杀 马 持 创 使 人   给 给\n",
      "3  南 元 侈 ， 实 计 上 世 哦 蜀 斯 入 不 入 算 供 货 了 ， 布 他 们 为 ...\n",
      "4  这 人 写 的 未 息 ， 又 俱 右 长 。 没 有 直 才 卖 学 ， 六 惠 准 六 官 ！\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "source": [
    "text_cleaning = lambda x: text_cleaning_chinese(x)\r\n",
    "test_ham['Cleaned_Text'] = pd.DataFrame(test_ham['Text'].apply(text_cleaning))\r\n",
    "test_spam['Cleaned_Text'] = pd.DataFrame(test_spam['Text'].apply(text_cleaning))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "source": [
    "pred_data = svm_model.predict(test_ham['Cleaned_Text'])\r\n",
    "test_ham['Predicted'] = pred_data\r\n",
    "test_ham['Predicted'].value_counts()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0    18416\n",
       "1     1583\n",
       "Name: Predicted, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 341
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "source": [
    "pred_data = svm_model.predict(test_spam['Cleaned_Text'])\r\n",
    "test_spam['Predicted'] = pred_data\r\n",
    "test_spam['Predicted'].value_counts()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1    15250\n",
       "0     2048\n",
       "Name: Predicted, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 342
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "test_ham1 = pd.read_csv('category/cleaned_ham_chinese.txt', delimiter= '\\t')\r\n",
    "test_ham1.columns = [\"Text\"]\r\n",
    "print(test_ham1.head())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "test_spam1 = pd.read_csv('category/cleaned_spam_chinese.txt', delimiter= '\\t')\r\n",
    "test_spam1.columns = [\"Text\"]\r\n",
    "print(test_spam1.head())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "text_cleaning = lambda x: text_cleaning_chinese(x)\r\n",
    "test_ham1['Cleaned_Text'] = pd.DataFrame(test_ham1['Text'].apply(text_cleaning))\r\n",
    "test_spam1['Cleaned_Text'] = pd.DataFrame(test_spam1['Text'].apply(text_cleaning))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pred_data = svm_model.predict(test_ham1['Cleaned_Text'])\r\n",
    "test_ham1['Predicted'] = pred_data\r\n",
    "test_ham1['Predicted'].value_counts()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pred_data = svm_model.predict(test_spam1['Cleaned_Text'])\r\n",
    "test_spam1['Predicted'] = pred_data\r\n",
    "test_spam1['Predicted'].value_counts()"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit ('venv': venv)"
  },
  "interpreter": {
   "hash": "063602af05503a7cdba16d07faccaa515e75478ba0f7dc2a4edde793b147bd70"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}