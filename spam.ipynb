{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pandas as pd \r\n",
    "import numpy as np\r\n",
    "# 1 spam, 0 ham"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# spam data english\r\n",
    "spam_data1 = pd.read_csv('category/spam_data_lg.csv')\r\n",
    "spam_data1.columns\r\n",
    "print(len(spam_data1))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "57534\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "spam_data1 = spam_data1.dropna(axis = 0, how ='any')\r\n",
    "len(spam_data1)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "57528"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "spam_data1 = spam_data1.drop(['Unnamed: 0'],axis=1)\r\n",
    "spam_data1.columns"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Index(['Text', 'Label[Spam/Non-Spam]'], dtype='object')"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "spam_data1.rename(columns = {'Label[Spam/Non-Spam]': 'Category'}, inplace = True)\r\n",
    "print(spam_data1.head())\r\n",
    "print(len(spam_data1))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                                                Text  Category\n",
      "0              That just seriously ruined my night..       0.0\n",
      "1  @JessleaC I no they are so funny ..! I love yo...       0.0\n",
      "2  @honkwas might drive so we could go subway or ...       0.0\n",
      "3  That @brad_frost post is bourne out by my expe...       0.0\n",
      "4       why tf do i keep waking up every 2 hours -.-       0.0\n",
      "57528\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "spam_data1['Category'] = spam_data1['Category'].astype(int)\r\n",
    "spam_data1.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                Text  Category\n",
       "0              That just seriously ruined my night..         0\n",
       "1  @JessleaC I no they are so funny ..! I love yo...         0\n",
       "2  @honkwas might drive so we could go subway or ...         0\n",
       "3  That @brad_frost post is bourne out by my expe...         0\n",
       "4       why tf do i keep waking up every 2 hours -.-         0"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>That just seriously ruined my night..</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@JessleaC I no they are so funny ..! I love yo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@honkwas might drive so we could go subway or ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>That @brad_frost post is bourne out by my expe...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>why tf do i keep waking up every 2 hours -.-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# spam data malay\r\n",
    "spam_data2 = pd.read_csv('category/spam_data_indon.csv')\r\n",
    "spam_data2.columns\r\n",
    "print(len(spam_data2))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1143\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "spam_data2 = spam_data2.dropna(axis = 0, how ='any')\r\n",
    "len(spam_data2)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1143"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "spam_data2.columns"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Index(['Teks', 'label'], dtype='object')"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "spam_data2.rename(columns = {'Teks': 'Text', 'label': 'Category'}, inplace = True)\r\n",
    "print(spam_data2.head())\r\n",
    "print(len(spam_data2))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                                                Text Category\n",
      "0  [PROMO] Beli paket Flash mulai 1GB di MY TELKO...    promo\n",
      "1  2.5 GB/30 hari hanya Rp 35 Ribu Spesial buat A...    promo\n",
      "2  2016-07-08 11:47:11.Plg Yth, sisa kuota Flash ...    promo\n",
      "3  2016-08-07 11:29:47.Plg Yth, sisa kuota Flash ...    promo\n",
      "4  4.5GB/30 hari hanya Rp 55 Ribu Spesial buat an...    promo\n",
      "1143\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "spam_data2['Category'].value_counts()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "normal      569\n",
       "penipuan    335\n",
       "promo       239\n",
       "Name: Category, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "spam_data2.loc[spam_data2['Category'] == 'promo', 'Category'] = 1\r\n",
    "spam_data2.loc[spam_data2['Category'] == 'penipuan', 'Category'] = 1\r\n",
    "spam_data2.loc[spam_data2['Category'] == 'normal', 'Category'] = 0"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "spam_data2['Category'] = spam_data2['Category'].astype(int)\r\n",
    "spam_data2.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                Text  Category\n",
       "0  [PROMO] Beli paket Flash mulai 1GB di MY TELKO...         1\n",
       "1  2.5 GB/30 hari hanya Rp 35 Ribu Spesial buat A...         1\n",
       "2  2016-07-08 11:47:11.Plg Yth, sisa kuota Flash ...         1\n",
       "3  2016-08-07 11:29:47.Plg Yth, sisa kuota Flash ...         1\n",
       "4  4.5GB/30 hari hanya Rp 55 Ribu Spesial buat an...         1"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[PROMO] Beli paket Flash mulai 1GB di MY TELKO...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.5 GB/30 hari hanya Rp 35 Ribu Spesial buat A...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-07-08 11:47:11.Plg Yth, sisa kuota Flash ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-08-07 11:29:47.Plg Yth, sisa kuota Flash ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.5GB/30 hari hanya Rp 55 Ribu Spesial buat an...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "# spam data chinese\r\n",
    "spam_data3 = pd.read_csv('category/spam_data_chinese_sm.csv')\r\n",
    "print(spam_data3.head())\r\n",
    "print(len(spam_data3))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                                                Text  Category\n",
      "0                          便 宜 又 适 用 ， 也 非 常 容 易 撕 掉         0\n",
      "1  质 量 跟 我 预 想 的 差 不 多 ， 布 料 很 薄 ， 说 不 上 有 没 有 色 ...         0\n",
      "2                    好 评 应 该 有 八 十 五 字 了 吧 ， 涨 淘 气 值         0\n",
      "3                          第 二 双 ， 超 级 棒 ， 上 脚 很 好 看         0\n",
      "4                差 ， 去 日 本 无 服 务 ， 耽 误 了 一 天 半 时 间 呵         0\n",
      "37297\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "spam_data3['Category'] = spam_data3['Category'].astype(int)\r\n",
    "spam_data3.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                Text  Category\n",
       "0                          便 宜 又 适 用 ， 也 非 常 容 易 撕 掉         0\n",
       "1  质 量 跟 我 预 想 的 差 不 多 ， 布 料 很 薄 ， 说 不 上 有 没 有 色 ...         0\n",
       "2                    好 评 应 该 有 八 十 五 字 了 吧 ， 涨 淘 气 值         0\n",
       "3                          第 二 双 ， 超 级 棒 ， 上 脚 很 好 看         0\n",
       "4                差 ， 去 日 本 无 服 务 ， 耽 误 了 一 天 半 时 间 呵         0"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>便 宜 又 适 用 ， 也 非 常 容 易 撕 掉</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>质 量 跟 我 预 想 的 差 不 多 ， 布 料 很 薄 ， 说 不 上 有 没 有 色 ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>好 评 应 该 有 八 十 五 字 了 吧 ， 涨 淘 气 值</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>第 二 双 ， 超 级 棒 ， 上 脚 很 好 看</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>差 ， 去 日 本 无 服 务 ， 耽 误 了 一 天 半 时 间 呵</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "# Combine all dataframes into one master dataframe\r\n",
    "spam_data = pd.concat([spam_data1, spam_data2], ignore_index = True)\r\n",
    "print(spam_data.head())\r\n",
    "print(len(spam_data))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                                                Text  Category\n",
      "0              That just seriously ruined my night..         0\n",
      "1  @JessleaC I no they are so funny ..! I love yo...         0\n",
      "2  @honkwas might drive so we could go subway or ...         0\n",
      "3  That @brad_frost post is bourne out by my expe...         0\n",
      "4       why tf do i keep waking up every 2 hours -.-         0\n",
      "58671\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "# spam_data = spam_data1.copy()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "# # spam data\r\n",
    "# spam_data2 = pd.read_csv('category/spam_data.csv')\r\n",
    "# spam_data2.columns\r\n",
    "# print(len(spam_data2))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "# spam_data2 = spam_data2.dropna(axis = 0, how ='any')\r\n",
    "# len(spam_data2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "# spam_data2.rename(columns = {'Message': 'Text'}, inplace = True)\r\n",
    "# print(spam_data2.head())\r\n",
    "# print(len(spam_data2))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "# spam_data2.loc[spam_data2['Category'] == 'spam', 'Category'] = 1\r\n",
    "# spam_data2.loc[spam_data2['Category'] == 'ham', 'Category'] = 0\r\n",
    "# spam_data2.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "# spam_data2['Category'] = spam_data2['Category'].astype(int)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "# #Combine all dataframes into one master dataframe\r\n",
    "# spam_data = pd.concat([spam_data1, spam_data2], ignore_index = True)\r\n",
    "# print(spam_data.head())\r\n",
    "# print(len(spam_data))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "# ## Drop missing value\r\n",
    "# spam_data = spam_data.dropna(axis = 0, how ='any')\r\n",
    "# len(spam_data)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "# spam_data['Category'] = spam_data['Category'].astype(int)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "spam_data['Category'].value_counts()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0    45757\n",
       "1    12914\n",
       "Name: Category, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "# from spacy.lang.en.stop_words import STOP_WORDS\r\n",
    "import stopwordsiso as sw\r\n",
    "from collections import Counter\r\n",
    "import string\r\n",
    "import spacy\r\n",
    "import re\r\n",
    "\r\n",
    "punctuation_auto = list(string.punctuation)\r\n",
    "punctuation_custom = [ '...', '…' ]\r\n",
    "punctuation = np.unique(punctuation_auto+punctuation_custom)\r\n",
    "\r\n",
    "stop_words = list(sw.stopwords([\"ms\", \"id\",\"zh\",\"en\"]))\r\n",
    "stop_words_custom = ['kau', 'yg', 'mcm', 'gak', 'nak', 'ni', 'tu', 'la', 'je', 'kat', 'ya', 'dgn', 'tau', 'org', 'rt', 'aja', 'nk', 'dah',\r\n",
    "                        'orang', 'sy', 'ga', 'kalo', 'kena']\r\n",
    "STOP_WORDS = np.unique(stop_words+stop_words_custom)\r\n",
    "\r\n",
    "def light_text_cleaning_english(text):\r\n",
    "    text = text.lower()\r\n",
    "    text = re.sub('\\[.*?\\]', '', text)\r\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\r\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\r\n",
    "    text = re.sub('[‘’“”…]', '', text)\r\n",
    "    text = re.sub('\\n', '', text)\r\n",
    "\r\n",
    "    # remove numbers\r\n",
    "    text = re.sub(r'\\d+', '', text)\r\n",
    "    # remove links\r\n",
    "    text = re.sub('http[s]?://\\S+', '', text)\r\n",
    "    # remove word with tweethandle @name\r\n",
    "    text = re.sub('[^ ]*@[^ ]*', '', text)\r\n",
    "\r\n",
    "    return text\r\n",
    "\r\n",
    "# def text_cleaning_english(text,mynlp):\r\n",
    "\r\n",
    "#     # remove numbers\r\n",
    "#     text = re.sub(r'\\d+', '', text)\r\n",
    "#     # remove links\r\n",
    "#     text = re.sub('http[s]?://\\S+', '', text)\r\n",
    "#     # remove word with tweethandle @name\r\n",
    "#     text = re.sub('[^ ]*@[^ ]*', '', text)\r\n",
    "\r\n",
    "#     doc = mynlp(text)\r\n",
    "    \r\n",
    "#     # Lemmatization\r\n",
    "#     tokens = []\r\n",
    "#     for token in doc:\r\n",
    "#         if token.lemma_ != \"-PRON-\":\r\n",
    "#             temp = token.lemma_.lower().strip()\r\n",
    "#         else:\r\n",
    "#             temp = token.lower_\r\n",
    "#         tokens.append(temp)\r\n",
    "    \r\n",
    "#     # Remove punctuation and stopwords\r\n",
    "#     cleaned_tokens = []\r\n",
    "#     for token in tokens:\r\n",
    "#         if token not in list(STOP_WORDS) and token not in punctuation:\r\n",
    "#             cleaned_tokens.append(token)\r\n",
    "\r\n",
    "#     # traverse in the string     \r\n",
    "#     complete_sentence = ' '.join([str(words) for words in cleaned_tokens])\r\n",
    "\r\n",
    "#     return complete_sentence\r\n",
    "\r\n",
    "def most_used_words(data,str_input):\r\n",
    "\r\n",
    "    words_list = []\r\n",
    "    temp = []\r\n",
    "    for index, row in data.iterrows():\r\n",
    "        # print(type(row['cleaned_'+str_input]))\r\n",
    "        tokens = str(row['Cleaned_'+str_input]).split() \r\n",
    "        for word in tokens:\r\n",
    "            if word not in list(STOP_WORDS):\r\n",
    "                words_list.append(word)\r\n",
    "\r\n",
    "    common_words = Counter(words_list).most_common(50)\r\n",
    "    for key, value in common_words:\r\n",
    "        temp.append({'name': key, 'value': value})\r\n",
    "\r\n",
    "    return temp"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "text_cleaning = lambda x: light_text_cleaning_english(x)\r\n",
    "spam_data['Cleaned_Text'] = pd.DataFrame(spam_data['Text'].apply(text_cleaning))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "# # get common words in data\r\n",
    "# word_list = most_used_words(spam_data,'Text')\r\n",
    "# print(word_list)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "# SPLIT TRAINING & TESTING DATA\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "\r\n",
    "X_train, X_test, y_train, y_test = train_test_split(spam_data['Cleaned_Text'],spam_data['Category'],test_size=0.2,shuffle=True, random_state=42)\r\n",
    "print(X_train.shape, y_train.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(46936,) (46936,)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "X_train.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "42194    i have cottage on my island now my island is e...\n",
       "354      rt strommark  word lead emerged from medieval ...\n",
       "30891    boot up google now v iphone pirate bay sets sa...\n",
       "21221                mom what did you learn in apush today\n",
       "13719    its a brutal experience to try and change the ...\n",
       "Name: Cleaned_Text, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "y_train.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "42194    1\n",
       "354      0\n",
       "30891    0\n",
       "21221    0\n",
       "13719    1\n",
       "Name: Category, dtype: int32"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
    "from sklearn.pipeline import Pipeline\r\n",
    "from sklearn.svm import LinearSVC \r\n",
    "from sklearn.linear_model import LogisticRegression\r\n",
    "from xgboost import XGBClassifier\r\n",
    "from sklearn.linear_model import SGDClassifier\r\n",
    "from sklearn.naive_bayes import MultinomialNB\r\n",
    "from sklearn.preprocessing import StandardScaler\r\n",
    "from sklearn.metrics import classification_report,confusion_matrix, accuracy_score, precision_score, f1_score, recall_score\r\n",
    "\r\n",
    "def sentiment_pipeline(data_train_input,data_train_target,model_type):\r\n",
    "    # Classifier selection\r\n",
    "    if model_type == \"linear\":\r\n",
    "        classifier = LinearSVC()\r\n",
    "    elif model_type == \"logistic\":\r\n",
    "        classifier = LogisticRegression(max_iter=1000)\r\n",
    "    elif model_type == \"sgd\":\r\n",
    "        classifier = SGDClassifier()\r\n",
    "    elif model_type == \"naive_bayes\":\r\n",
    "        classifier = MultinomialNB()\r\n",
    "    elif model_type == \"xgboost\":\r\n",
    "        classifier = XGBClassifier(use_label_encoder=False,eta=0.1,gamma=0.3, n_estimators=100, learning_rate=0.5, min_child_weight=5, \r\n",
    "        max_depth=5, colsample_bytree=0.7,objective=\"binary:logistic\", eval_metric=\"logloss\",verbosity=0)\r\n",
    "\r\n",
    "    tfidf = TfidfVectorizer()\r\n",
    "\r\n",
    "    # Pipeline setup\r\n",
    "    clf = Pipeline([('tfidf', tfidf), ('clf', classifier)])\r\n",
    "\r\n",
    "    model = clf.fit(data_train_input,data_train_target)\r\n",
    "\r\n",
    "    return model\r\n",
    "\r\n",
    "def sentiment_model_predict(model,data_test_input,data_test_target):\r\n",
    "    data_prediction=model.predict(data_test_input)\r\n",
    "    conf_matrix = confusion_matrix(data_test_target,data_prediction)\r\n",
    "    acc_score = accuracy_score(data_test_target, data_prediction)\r\n",
    "    pre_score = precision_score(data_test_target, data_prediction, average=\"macro\")\r\n",
    "    re_score = recall_score(data_test_target, data_prediction, average=\"macro\")\r\n",
    "    f_score = f1_score(data_test_target, data_prediction, average=\"macro\")\r\n",
    "\r\n",
    "    print(\"Accuracy : \"+str(round(acc_score*100,2)))\r\n",
    "    print(\"Precision : \"+str(round(pre_score*100,2)))\r\n",
    "    print(\"Recall : \"+str(round(re_score*100,2)))\r\n",
    "    print(\"F1-Score :\"+str(round(f_score*100,2)))\r\n",
    "    print(conf_matrix)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "# Support Vector Classification\r\n",
    "svm_model = sentiment_pipeline(X_train, y_train, 'linear')\r\n",
    "sentiment_model_predict(svm_model,X_test,y_test)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy : 93.42\n",
      "Precision : 92.43\n",
      "Recall : 87.48\n",
      "F1-Score :89.65\n",
      "[[9023  195]\n",
      " [ 577 1940]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "# test_data1 = pd.read_csv('category/spam_test.txt', delimiter= '\\t')\r\n",
    "# test_data1.columns = [\"Text\"]\r\n",
    "# print(test_data1.head())\r\n",
    "# print(len(test_data1))\r\n",
    "\r\n",
    "# test_data2 = pd.read_csv('category/ham_test.txt', delimiter= '\\t')\r\n",
    "# test_data2.columns = [\"Text\"]\r\n",
    "# print(test_data2.head())\r\n",
    "# print(len(test_data2))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "# text_cleaning = lambda x: light_text_cleaning_english(x)\r\n",
    "# test_data1['Cleaned_Text'] = pd.DataFrame(test_data1['Text'].apply(text_cleaning))\r\n",
    "# test_data2['Cleaned_Text'] = pd.DataFrame(test_data2['Text'].apply(text_cleaning))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "# pred_data=svm_model.predict(test_data1['Cleaned_Text'])\r\n",
    "# test_data1['Predicted'] = pred_data\r\n",
    "# test_data1['Predicted'].value_counts()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "# pred_data=svm_model.predict(test_data2['Cleaned_Text'])\r\n",
    "# test_data2['Predicted'] = pred_data\r\n",
    "# test_data2['Predicted'].value_counts()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "# Logistic Regression\r\n",
    "lr_model = sentiment_pipeline(X_train, y_train, 'logistic')\r\n",
    "sentiment_model_predict(lr_model,X_test,y_test)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy : 91.9\n",
      "Precision : 92.57\n",
      "Recall : 82.78\n",
      "F1-Score :86.5\n",
      "[[9102  116]\n",
      " [ 835 1682]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "# Stochastic Gradient Descent\r\n",
    "sgd_model = sentiment_pipeline(X_train, y_train, 'sgd')\r\n",
    "sentiment_model_predict(sgd_model,X_test,y_test)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy : 90.79\n",
      "Precision : 92.85\n",
      "Recall : 79.51\n",
      "F1-Score :83.99\n",
      "[[9150   68]\n",
      " [1013 1504]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "# Multinomial Naive Bayes\r\n",
    "nb_model = sentiment_pipeline(X_train, y_train, 'naive_bayes')\r\n",
    "sentiment_model_predict(nb_model,X_test,y_test)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy : 88.2\n",
      "Precision : 92.54\n",
      "Recall : 72.83\n",
      "F1-Score :77.77\n",
      "[[9194   24]\n",
      " [1361 1156]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "# Xgboost\r\n",
    "# multi:softprob/multi:softmax for multi class and binary:logistic for binary\r\n",
    "xg_model = sentiment_pipeline(X_train, y_train, 'xgboost')\r\n",
    "sentiment_model_predict(xg_model,X_test,y_test)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy : 90.88\n",
      "Precision : 91.02\n",
      "Recall : 80.97\n",
      "F1-Score :84.69\n",
      "[[9064  154]\n",
      " [ 916 1601]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "# import joblib\r\n",
    "\r\n",
    "# # save model\r\n",
    "# joblib_file_svm = \"linear_pretrained_multi_spam_model.pkl\"  \r\n",
    "# joblib.dump(svm_model, joblib_file_svm)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "# # load model\r\n",
    "# joblib_SVM_model = joblib.load(joblib_file_svm)\r\n",
    "# sentiment_model_predict(joblib_SVM_model,X_test,y_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "# print(spam_data.loc[spam_data['Category'] == 1])"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit ('venv': venv)"
  },
  "interpreter": {
   "hash": "ff3bca78fb5aa06f77b0acc16fb034ebfdaa95e6def02107f20b05109b7a797f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}