{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "source": [
    "import pandas as pd \r\n",
    "import numpy as np \r\n",
    "# spam 1, ham 0"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "source": [
    "ham_data = pd.read_csv('category/cleaned_ham_chinese.txt', delimiter= '\\t')\r\n",
    "ham_data.columns = [\"Text\"]\r\n",
    "ham_data.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                  Text\n",
       "0      带 给 我 们 大 常 州 一 场 壮 观 的 视 觉 盛 宴\n",
       "1            有 原 因 不 明 的 泌 尿 系 统 结 石 等\n",
       "2            年 从 盐 城 拉 回 来 的 麻 麻 的 嫁 妆\n",
       "3                感 到 自 减 肥 跳 减 肥 健 美 操\n",
       "4  这 款 智 能 杀 菌 机 器 人 是 扫 地 机 的 最 佳 伴 侣"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>带 给 我 们 大 常 州 一 场 壮 观 的 视 觉 盛 宴</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>有 原 因 不 明 的 泌 尿 系 统 结 石 等</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>年 从 盐 城 拉 回 来 的 麻 麻 的 嫁 妆</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>感 到 自 减 肥 跳 减 肥 健 美 操</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>这 款 智 能 杀 菌 机 器 人 是 扫 地 机 的 最 佳 伴 侣</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 52
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "source": [
    "ham_data['Category'] = 0\r\n",
    "ham_data.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                  Text  Category\n",
       "0      带 给 我 们 大 常 州 一 场 壮 观 的 视 觉 盛 宴         0\n",
       "1            有 原 因 不 明 的 泌 尿 系 统 结 石 等         0\n",
       "2            年 从 盐 城 拉 回 来 的 麻 麻 的 嫁 妆         0\n",
       "3                感 到 自 减 肥 跳 减 肥 健 美 操         0\n",
       "4  这 款 智 能 杀 菌 机 器 人 是 扫 地 机 的 最 佳 伴 侣         0"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>带 给 我 们 大 常 州 一 场 壮 观 的 视 觉 盛 宴</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>有 原 因 不 明 的 泌 尿 系 统 结 石 等</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>年 从 盐 城 拉 回 来 的 麻 麻 的 嫁 妆</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>感 到 自 减 肥 跳 减 肥 健 美 操</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>这 款 智 能 杀 菌 机 器 人 是 扫 地 机 的 最 佳 伴 侣</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 53
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "source": [
    "len(ham_data)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "718717"
      ]
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "source": [
    "ham_data = ham_data.head(100000)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "source": [
    "spam_data = pd.read_csv('category/cleaned_spam_chinese.txt', delimiter= '\\t')\r\n",
    "spam_data.columns = [\"Text\"]\r\n",
    "spam_data.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                Text\n",
       "0  感 谢 致 电 杭 州 萧 山 全 金 釜 韩 国 烧 烤 店 本 店 位 于 金 城 路 ...\n",
       "1  一 次 价 值 元 王 牌 项 目 可 充 值 元 店 内 项 目 卡 一 张 可 以 参 ...\n",
       "2      长 期 诚 信 在 本 市 作 各 类 资 格 职 称 以 及 印 章 牌 等 祥 李 伟\n",
       "3  依 林 美 容 三 八 女 人 节 倾 情 大 放 送 活 动 开 始 啦 超 值 套 餐 ...\n",
       "4  红 都 百 货 楼 婷 美 专 柜 节 活 动 火 热 进 行 中 一 年 仅 一 次 的 ..."
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>感 谢 致 电 杭 州 萧 山 全 金 釜 韩 国 烧 烤 店 本 店 位 于 金 城 路 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>一 次 价 值 元 王 牌 项 目 可 充 值 元 店 内 项 目 卡 一 张 可 以 参 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>长 期 诚 信 在 本 市 作 各 类 资 格 职 称 以 及 印 章 牌 等 祥 李 伟</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>依 林 美 容 三 八 女 人 节 倾 情 大 放 送 活 动 开 始 啦 超 值 套 餐 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>红 都 百 货 楼 婷 美 专 柜 节 活 动 火 热 进 行 中 一 年 仅 一 次 的 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "source": [
    "spam_data['Category'] = 1\r\n",
    "spam_data.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                Text  Category\n",
       "0  感 谢 致 电 杭 州 萧 山 全 金 釜 韩 国 烧 烤 店 本 店 位 于 金 城 路 ...         1\n",
       "1  一 次 价 值 元 王 牌 项 目 可 充 值 元 店 内 项 目 卡 一 张 可 以 参 ...         1\n",
       "2      长 期 诚 信 在 本 市 作 各 类 资 格 职 称 以 及 印 章 牌 等 祥 李 伟         1\n",
       "3  依 林 美 容 三 八 女 人 节 倾 情 大 放 送 活 动 开 始 啦 超 值 套 餐 ...         1\n",
       "4  红 都 百 货 楼 婷 美 专 柜 节 活 动 火 热 进 行 中 一 年 仅 一 次 的 ...         1"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>感 谢 致 电 杭 州 萧 山 全 金 釜 韩 国 烧 烤 店 本 店 位 于 金 城 路 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>一 次 价 值 元 王 牌 项 目 可 充 值 元 店 内 项 目 卡 一 张 可 以 参 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>长 期 诚 信 在 本 市 作 各 类 资 格 职 称 以 及 印 章 牌 等 祥 李 伟</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>依 林 美 容 三 八 女 人 节 倾 情 大 放 送 活 动 开 始 啦 超 值 套 餐 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>红 都 百 货 楼 婷 美 专 柜 节 活 动 火 热 进 行 中 一 年 仅 一 次 的 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 57
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "source": [
    "len(spam_data)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "79957"
      ]
     },
     "metadata": {},
     "execution_count": 58
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "source": [
    "# Combine all dataframes into one master dataframe\r\n",
    "spam_data = pd.concat([ham_data, spam_data], ignore_index = True)\r\n",
    "print(spam_data.head())\r\n",
    "print(len(spam_data))\r\n",
    "spam_data.head()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                                  Text  Category\n",
      "0      带 给 我 们 大 常 州 一 场 壮 观 的 视 觉 盛 宴         0\n",
      "1            有 原 因 不 明 的 泌 尿 系 统 结 石 等         0\n",
      "2            年 从 盐 城 拉 回 来 的 麻 麻 的 嫁 妆         0\n",
      "3                感 到 自 减 肥 跳 减 肥 健 美 操         0\n",
      "4  这 款 智 能 杀 菌 机 器 人 是 扫 地 机 的 最 佳 伴 侣         0\n",
      "179957\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                  Text  Category\n",
       "0      带 给 我 们 大 常 州 一 场 壮 观 的 视 觉 盛 宴         0\n",
       "1            有 原 因 不 明 的 泌 尿 系 统 结 石 等         0\n",
       "2            年 从 盐 城 拉 回 来 的 麻 麻 的 嫁 妆         0\n",
       "3                感 到 自 减 肥 跳 减 肥 健 美 操         0\n",
       "4  这 款 智 能 杀 菌 机 器 人 是 扫 地 机 的 最 佳 伴 侣         0"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>带 给 我 们 大 常 州 一 场 壮 观 的 视 觉 盛 宴</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>有 原 因 不 明 的 泌 尿 系 统 结 石 等</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>年 从 盐 城 拉 回 来 的 麻 麻 的 嫁 妆</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>感 到 自 减 肥 跳 减 肥 健 美 操</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>这 款 智 能 杀 菌 机 器 人 是 扫 地 机 的 最 佳 伴 侣</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 59
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "source": [
    "spam_data['Category'].value_counts()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0    100000\n",
       "1     79957\n",
       "Name: Category, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 60
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "source": [
    "spam_data = spam_data.dropna(axis = 0, how ='any')\r\n",
    "len(spam_data)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "179957"
      ]
     },
     "metadata": {},
     "execution_count": 61
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "source": [
    "# from spacy.lang.en.stop_words import STOP_WORDS\r\n",
    "import stopwordsiso as sw\r\n",
    "from collections import Counter\r\n",
    "import string\r\n",
    "import spacy\r\n",
    "import re\r\n",
    "from zhon.hanzi import non_stops, stops\r\n",
    "import jieba.posseg as pseg\r\n",
    "\r\n",
    "punctuation_auto = list(string.punctuation)\r\n",
    "punctuation_ch = list(stops+non_stops)\r\n",
    "punctuation_custom = [ '...', '…' ]\r\n",
    "punctuation = np.unique(punctuation_auto+punctuation_ch+punctuation_custom)\r\n",
    "\r\n",
    "stop_words = list(sw.stopwords([\"zh\"]))\r\n",
    "stop_words_custom = ['kau', 'yg', 'mcm', 'gak', 'nak', 'ni', 'tu', 'la', 'je', 'kat', 'ya', 'dgn', 'tau', 'org', 'rt', 'aja', 'nk', 'dah',\r\n",
    "                        'orang', 'sy', 'ga', 'kalo', 'kena']\r\n",
    "STOP_WORDS = np.unique(stop_words+stop_words_custom)\r\n",
    "\r\n",
    "def text_cleaning_chinese(text):\r\n",
    "    \r\n",
    "    text= text.replace(\" \", \"\")\r\n",
    "\r\n",
    "    words = pseg.cut(text)\r\n",
    "    cleaned_tokens = []\r\n",
    "    for w in words:\r\n",
    "        if w.word not in list(STOP_WORDS) and w.word not in list(punctuation):\r\n",
    "            cleaned_tokens.append(w.word)\r\n",
    "\r\n",
    "    complete_sentence = ' '.join([str(words) for words in cleaned_tokens])\r\n",
    "\r\n",
    "    return complete_sentence"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "source": [
    "text_cleaning = lambda x: text_cleaning_chinese(x)\r\n",
    "spam_data['Cleaned_Text'] = pd.DataFrame(spam_data['Text'].apply(text_cleaning))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\MUHAMM~1.ADL\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.253 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "source": [
    "# SPLIT TRAINING & TESTING DATA\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "\r\n",
    "X_train, X_test, y_train, y_test = train_test_split(spam_data['Cleaned_Text'],spam_data['Category'],test_size=0.2,shuffle=True, random_state=42)\r\n",
    "print(X_train.shape, y_train.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(143965,) (143965,)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
    "from sklearn.pipeline import Pipeline\r\n",
    "from sklearn.svm import LinearSVC \r\n",
    "from sklearn.linear_model import LogisticRegression\r\n",
    "from xgboost import XGBClassifier\r\n",
    "from sklearn.linear_model import SGDClassifier\r\n",
    "from sklearn.naive_bayes import MultinomialNB\r\n",
    "from sklearn.preprocessing import StandardScaler\r\n",
    "from sklearn.metrics import classification_report,confusion_matrix, accuracy_score, precision_score, f1_score, recall_score\r\n",
    "\r\n",
    "def sentiment_pipeline(data_train_input,data_train_target,model_type):\r\n",
    "    # Classifier selection\r\n",
    "    if model_type == \"linear\":\r\n",
    "        classifier = LinearSVC()\r\n",
    "    elif model_type == \"logistic\":\r\n",
    "        classifier = LogisticRegression(max_iter=1000)\r\n",
    "    elif model_type == \"sgd\":\r\n",
    "        classifier = SGDClassifier()\r\n",
    "    elif model_type == \"naive_bayes\":\r\n",
    "        classifier = MultinomialNB()\r\n",
    "    elif model_type == \"xgboost\":\r\n",
    "        classifier = XGBClassifier(use_label_encoder=False,eta=0.1,gamma=0.3, n_estimators=100, learning_rate=0.5, min_child_weight=5, \r\n",
    "        max_depth=5, colsample_bytree=0.7,objective=\"binary:logistic\", eval_metric=\"logloss\",verbosity=0)\r\n",
    "\r\n",
    "    tfidf = TfidfVectorizer()\r\n",
    "\r\n",
    "    # Pipeline setup\r\n",
    "    clf = Pipeline([('tfidf', tfidf), ('clf', classifier)])\r\n",
    "\r\n",
    "    model = clf.fit(data_train_input,data_train_target)\r\n",
    "\r\n",
    "    return model\r\n",
    "\r\n",
    "def sentiment_model_predict(model,data_test_input,data_test_target):\r\n",
    "    data_prediction=model.predict(data_test_input)\r\n",
    "    conf_matrix = confusion_matrix(data_test_target,data_prediction)\r\n",
    "    acc_score = accuracy_score(data_test_target, data_prediction)\r\n",
    "    pre_score = precision_score(data_test_target, data_prediction, average=\"macro\")\r\n",
    "    re_score = recall_score(data_test_target, data_prediction, average=\"macro\")\r\n",
    "    f_score = f1_score(data_test_target, data_prediction, average=\"macro\")\r\n",
    "\r\n",
    "    print(\"Accuracy : \"+str(round(acc_score*100,2)))\r\n",
    "    print(\"Precision : \"+str(round(pre_score*100,2)))\r\n",
    "    print(\"Recall : \"+str(round(re_score*100,2)))\r\n",
    "    print(\"F1-Score :\"+str(round(f_score*100,2)))\r\n",
    "    print(conf_matrix)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "source": [
    "# Support Vector Classification\r\n",
    "svm_model = sentiment_pipeline(X_train, y_train, 'linear')\r\n",
    "sentiment_model_predict(svm_model,X_test,y_test)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy : 97.82\n",
      "Precision : 97.84\n",
      "Recall : 97.75\n",
      "F1-Score :97.79\n",
      "[[19583   317]\n",
      " [  469 15623]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "source": [
    "lg_model = sentiment_pipeline(X_train, y_train, 'logistic')\r\n",
    "sentiment_model_predict(lg_model,X_test,y_test)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy : 97.14\n",
      "Precision : 97.21\n",
      "Recall : 97.0\n",
      "F1-Score :97.1\n",
      "[[19557   343]\n",
      " [  687 15405]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "source": [
    "# save model\r\n",
    "import joblib\r\n",
    "\r\n",
    "# Save RL_Model to file in the current working directory\r\n",
    "joblib.dump(svm_model, \"spam_svm_chinese_newmodel.pkl\")\r\n",
    "joblib.dump(lg_model, \"spam_svm_chinese_newmodel.pkl\")"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['spam_svm_chinese_newmodel.pkl']"
      ]
     },
     "metadata": {},
     "execution_count": 68
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# For model testing"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pandas as pd \r\n",
    "import numpy as np \r\n",
    "import joblib \r\n",
    "\r\n",
    "# from spacy.lang.en.stop_words import STOP_WORDS\r\n",
    "import stopwordsiso as sw\r\n",
    "from collections import Counter\r\n",
    "import string\r\n",
    "import spacy\r\n",
    "import re\r\n",
    "from zhon.hanzi import non_stops, stops\r\n",
    "import jieba.posseg as pseg\r\n",
    "\r\n",
    "punctuation_auto = list(string.punctuation)\r\n",
    "punctuation_ch = list(stops+non_stops)\r\n",
    "punctuation_custom = [ '...', '…' ]\r\n",
    "punctuation = np.unique(punctuation_auto+punctuation_ch+punctuation_custom)\r\n",
    "\r\n",
    "stop_words = list(sw.stopwords([\"zh\"]))\r\n",
    "stop_words_custom = ['kau', 'yg', 'mcm', 'gak', 'nak', 'ni', 'tu', 'la', 'je', 'kat', 'ya', 'dgn', 'tau', 'org', 'rt', 'aja', 'nk', 'dah',\r\n",
    "                        'orang', 'sy', 'ga', 'kalo', 'kena']\r\n",
    "STOP_WORDS = np.unique(stop_words+stop_words_custom)\r\n",
    "\r\n",
    "def text_cleaning_chinese(text):\r\n",
    "    \r\n",
    "    text= text.replace(\" \", \"\")\r\n",
    "\r\n",
    "    words = pseg.cut(text)\r\n",
    "    cleaned_tokens = []\r\n",
    "    for w in words:\r\n",
    "        if w.word not in list(STOP_WORDS) and w.word not in list(punctuation):\r\n",
    "            cleaned_tokens.append(w.word)\r\n",
    "\r\n",
    "    complete_sentence = ' '.join([str(words) for words in cleaned_tokens])\r\n",
    "\r\n",
    "    return complete_sentence\r\n",
    "\r\n",
    "# load model\r\n",
    "joblib_SVM_model = joblib.load(\"model/spam_svm_chinese_newmodel.pkl\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "test_ham = pd.read_csv('category/ham_test.txt', delimiter= '\\t')\r\n",
    "test_ham.columns = [\"Text\"]\r\n",
    "print(test_ham.head())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                                                Text\n",
      "0                                才 买 了 没 好 久 就 降 价 了\n",
      "1  是 假 货 ， 味 道 一 点 也 不 好 ， 赚 黑 钱 的 ， 亲 们 千 万 别 买 ...\n",
      "2                杆 子 是 干 大 物 滴 … … 这 个 价 位 里 … 很 牛 逼\n",
      "3                                        款 式 简 单 舒 适\n",
      "4                                        真 的 灰 常 好 看\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "test_spam = pd.read_csv('category/spam_test.txt', delimiter= '\\t')\r\n",
    "test_spam.columns = [\"Text\"]\r\n",
    "print(test_spam.head())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                                                Text\n",
      "0                                      图 腾 ？ ？ ？ 狗 尼\n",
      "1                      到 处 视 始 的 宝 家 有 啥 式 好 旗 由 的 ？\n",
      "2                                  杀 马 持 创 使 人   给 给\n",
      "3  南 元 侈 ， 实 计 上 世 哦 蜀 斯 入 不 入 算 供 货 了 ， 布 他 们 为 ...\n",
      "4  这 人 写 的 未 息 ， 又 俱 右 长 。 没 有 直 才 卖 学 ， 六 惠 准 六 官 ！\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "text_cleaning = lambda x: text_cleaning_chinese(x)\r\n",
    "test_ham['Cleaned_Text'] = pd.DataFrame(test_ham['Text'].apply(text_cleaning))\r\n",
    "test_spam['Cleaned_Text'] = pd.DataFrame(test_spam['Text'].apply(text_cleaning))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\MUHAMM~1.ADL\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.738 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "pred_data = joblib_SVM_model.predict(test_ham['Cleaned_Text'])\r\n",
    "test_ham['Predicted'] = pred_data\r\n",
    "test_ham['Predicted'].value_counts()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0    19327\n",
       "1      672\n",
       "Name: Predicted, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "pred_data = joblib_SVM_model.predict(test_spam['Cleaned_Text'])\r\n",
    "test_spam['Predicted'] = pred_data\r\n",
    "test_spam['Predicted'].value_counts()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0    17093\n",
       "1      205\n",
       "Name: Predicted, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit ('venv': venv)"
  },
  "interpreter": {
   "hash": "ff3bca78fb5aa06f77b0acc16fb034ebfdaa95e6def02107f20b05109b7a797f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}